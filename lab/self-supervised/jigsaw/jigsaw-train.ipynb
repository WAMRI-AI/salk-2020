{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from JigsawDataLoader import DataLoader\n",
    "from JigsawNetwork import Network\n",
    "\n",
    "from copy import copy\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "gpu_id = 2\n",
    "num_cores = 4\n",
    "torch.cuda.set_device(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_dl(ds, bs, pct=0.1, seed=None):\n",
    "    \"\"\"Takes a databunch as input and returns a mini-version of the dataset\n",
    "    This is useful for debugging and rapid experimentation. \n",
    "    data -> a databunch object\n",
    "    pct  -> the fraction of original dataset size (default: 0.1)\"\"\"\n",
    "    if seed: np.random.seed(seed)\n",
    "    size = len(ds)\n",
    "    indices = np.random.choice(np.arange(size), \n",
    "                                   size=int(pct*size), replace=False)\n",
    "    sampler = torch.utils.data.sampler.SubsetRandomSampler(indices)\n",
    "    mini_dl = torch.utils.data.DataLoader(dataset=ds,\n",
    "                                            batch_size=bs,\n",
    "                                            sampler=sampler,\n",
    "                                            num_workers=4)\n",
    "    return mini_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pth = pd.read_csv('../train.csv')\n",
    "train_ds = DataLoader(train_pth)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_ds,\n",
    "                                            batch_size=bs,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=num_cores)\n",
    "sub_train_loader = subsample_dl(ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79997, 2500, 250)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(train_loader), len(sub_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pth = pd.read_csv('../valid.csv')\n",
    "val_ds = DataLoader(val_pth)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_ds,\n",
    "                                            batch_size=bs,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=num_cores)\n",
    "sub_val_loader = subsample_dl(val_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 625, 63)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_ds), len(val_loader), len(sub_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sub_train_loader\n",
    "val_data = sub_val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: add validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_per_epoch = len(train_data)\n",
    "# print('Images: train %d, validation %d'%(train_data.N,val_data.N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss func and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=lr,momentum=0.9,weight_decay = 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [01:20,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:33,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:19, 13.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:19, 13.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:18, 13.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:19, 12.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:19, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:19, 12.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:19, 12.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:19, 12.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:19, 12.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:19, 12.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "250it [00:19, 12.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 6.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_time, net_time = [], []\n",
    "for epoch in range(30):\n",
    "#         if epoch%10==0 and epoch>0:\n",
    "#             test(net,criterion,logger_test,val_loader,steps)\n",
    "#         lr = adjust_learning_rate(optimizer, epoch, init_lr=args.lr, step=20, decay=0.1)\n",
    "        net.train()\n",
    "        total_loss = 0\n",
    "        total_items = 0\n",
    "        for i, (images, labels) in enumerate(progress_bar(train_data)):\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()          \n",
    "            outputs = net(images)\n",
    "            \n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()*len(labels)\n",
    "            total_items += len(labels)\n",
    "            \n",
    "        train_loss = total_loss / total_items\n",
    "        print(f'Train loss {train_loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_metrics(model, valid_dl):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_true = 0\n",
    "    total_items = 0\n",
    "    for images, y in valid_dl:\n",
    "        images = Variable(images)\n",
    "        images = images.cuda()\n",
    "        out = model(images)\n",
    "        out = out.cpu().data\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        total_loss += loss.item()*len(y)\n",
    "        total_true += (out.argmax(dim=1) == y).sum().item()\n",
    "        total_items += len(y)\n",
    "    val_loss = total_loss / total_items\n",
    "    val_acc = total_true / total_items\n",
    "    print(f'Valid loss {val_loss:.2f}')\n",
    "    print(f'Accuracy: {val_acc:.2f}')\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss 6.72\n",
      "Accuracy: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.723890724182129, 0.004)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_metrics(net, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, numpy as np\n",
    "import argparse\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# import tensorflow # needs to call tensorflow before torch, otherwise crush #???\n",
    "# sys.path.append('Utils')\n",
    "# from logger import Logger\n",
    "# from TrainingUtils import adjust_learning_rate, compute_accuracy\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Train JigsawPuzzleSolver on Imagenet')\n",
    "parser.add_argument('data', type=str, help='Path to Imagenet folder')\n",
    "parser.add_argument('--model', default=None, type=str, help='Path to pretrained model')\n",
    "parser.add_argument('--classes', default=1000, type=int, help='Number of permutation to use')\n",
    "parser.add_argument('--gpu', default=0, type=int, help='gpu id')\n",
    "parser.add_argument('--epochs', default=70, type=int, help='number of total epochs for training')\n",
    "parser.add_argument('--iter_start', default=0, type=int, help='Starting iteration count')\n",
    "parser.add_argument('--batch', default=256, type=int, help='batch size')\n",
    "parser.add_argument('--checkpoint', default='checkpoints/', type=str, help='checkpoint folder')\n",
    "parser.add_argument('--lr', default=0.001, type=float, help='learning rate for SGD optimizer')\n",
    "parser.add_argument('--cores', default=0, type=int, help='number of CPU core for loading')\n",
    "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                    help='evaluate model on validation set, No training')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "# data loader\n",
    "    train_data = DataLoader(trainpath,args.data+'/ilsvrc12_train.txt',\n",
    "                            classes=args.classes)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                            batch_size=args.batch,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=args.cores)\n",
    "    \n",
    "    valpath = args.data+'/ILSVRC2012_img_val'\n",
    "    if os.path.exists(valpath+'_255x255'):\n",
    "        valpath += '_255x255'\n",
    "    val_data = DataLoader(valpath, args.data+'/ilsvrc12_val.txt',\n",
    "                            classes=args.classes)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_data,\n",
    "                                            batch_size=args.batch,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=args.cores)\n",
    "# iterating through data\n",
    "    \n",
    "    N = train_data.N\n",
    "    \n",
    "    iter_per_epoch = train_data.N/args.batch\n",
    "    print('Images: train %d, validation %d'%(train_data.N,val_data.N))\n",
    "    \n",
    "# Network initialize\n",
    "    net = Network(args.classes)\n",
    "    if args.gpu is not None:\n",
    "        net.cuda()\n",
    "    \n",
    "    ############## Load from checkpoint if exists, otherwise from model ###############\n",
    "    if os.path.exists(args.checkpoint):\n",
    "        files = [f for f in os.listdir(args.checkpoint) if 'pth' in f]\n",
    "        if len(files)>0:\n",
    "            files.sort()\n",
    "            #print files\n",
    "            ckp = files[-1]\n",
    "            net.load_state_dict(torch.load(args.checkpoint+'/'+ckp))\n",
    "            args.iter_start = int(ckp.split(\".\")[-3].split(\"_\")[-1])\n",
    "            print('Starting from: ',ckp)\n",
    "        else:\n",
    "            if args.model is not None:\n",
    "                net.load(args.model)\n",
    "    else:\n",
    "        if args.model is not None:\n",
    "            net.load(args.model)\n",
    "\n",
    "# loss func and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(),lr=args.lr,momentum=0.9,weight_decay = 5e-4)\n",
    "    \n",
    "# dunno how to use the logger yet\n",
    "#     logger = Logger(args.checkpoint+'/train')\n",
    "#     logger_test = Logger(args.checkpoint+'/test')\n",
    "    \n",
    "    ############## TESTING ###############\n",
    "    if args.evaluate:\n",
    "        test(net,criterion,None,val_loader,0)\n",
    "        return\n",
    "    \n",
    "    ############## TRAINING ###############\n",
    "    print(('Start training: lr %f, batch size %d, classes %d'%(args.lr,args.batch,args.classes)))\n",
    "    print(('Checkpoint: '+args.checkpoint))\n",
    "    \n",
    "    # Train the Model\n",
    "    batch_time, net_time = [], []\n",
    "    steps = args.iter_start\n",
    "    for epoch in range(int(args.iter_start/iter_per_epoch),args.epochs):\n",
    "        if epoch%10==0 and epoch>0:\n",
    "            test(net,criterion,logger_test,val_loader,steps)\n",
    "        lr = adjust_learning_rate(optimizer, epoch, init_lr=args.lr, step=20, decay=0.1)\n",
    "        \n",
    "        end = time()\n",
    "        for i, (images, labels, original) in enumerate(train_loader):\n",
    "            batch_time.append(time()-end)\n",
    "            if len(batch_time)>100:\n",
    "                del batch_time[0]\n",
    "            \n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if args.gpu is not None:\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            t = time()\n",
    "            outputs = net(images)\n",
    "            net_time.append(time()-t)\n",
    "            if len(net_time)>100:\n",
    "                del net_time[0]\n",
    "            \n",
    "            prec1, prec5 = compute_accuracy(outputs.cpu().data, labels.cpu().data, topk=(1, 5))\n",
    "            acc = prec1[0]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = float(loss.cpu().data.numpy())\n",
    "\n",
    "            if steps%20==0:\n",
    "                print(('[%2d/%2d] %5d) [batch load % 2.3fsec, net %1.2fsec], LR %.5f, Loss: % 1.3f, Accuracy % 2.2f%%' %(\n",
    "                            epoch+1, args.epochs, steps, \n",
    "                            np.mean(batch_time), np.mean(net_time),\n",
    "                            lr, loss,acc)))\n",
    "\n",
    "            if steps%20==0:\n",
    "                logger.scalar_summary('accuracy', acc, steps)\n",
    "                logger.scalar_summary('loss', loss, steps)\n",
    "                \n",
    "                original = [im[0] for im in original]\n",
    "                imgs = np.zeros([9,75,75,3])\n",
    "                for ti, img in enumerate(original):\n",
    "                    img = img.numpy()\n",
    "                    imgs[ti] = np.stack([(im-im.min())/(im.max()-im.min()) \n",
    "                                         for im in img],axis=2)\n",
    "                \n",
    "                logger.image_summary('input', imgs, steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if steps%1000==0:\n",
    "                filename = '%s/jps_%03i_%06d.pth.tar'%(args.checkpoint,epoch,steps)\n",
    "                net.save(filename)\n",
    "                print('Saved: '+args.checkpoint)\n",
    "            \n",
    "            end = time()\n",
    "\n",
    "        if os.path.exists(args.checkpoint+'/stop.txt'):\n",
    "            # break without using CTRL+C\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net,criterion,logger,val_loader,steps):\n",
    "    print('Evaluating network.......')\n",
    "    accuracy = []\n",
    "    net.eval()\n",
    "    for i, (images, labels, _) in enumerate(val_loader):\n",
    "        images = Variable(images)\n",
    "        if args.gpu is not None:\n",
    "            images = images.cuda()\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        outputs = net(images)\n",
    "        outputs = outputs.cpu().data\n",
    "\n",
    "        prec1, prec5 = compute_accuracy(outputs, labels, topk=(1, 5))\n",
    "        accuracy.append(prec1[0])\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.scalar_summary('accuracy', np.mean(accuracy), steps)\n",
    "    print('TESTING: %d), Accuracy %.2f%%' %(steps,np.mean(accuracy)))\n",
    "    net.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Salk)",
   "language": "python",
   "name": "salk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
